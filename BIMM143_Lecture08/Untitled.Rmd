---
title: "class08"
author: "Xuqian Tan"
date: "2/5/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster?
Q. What ‘component’ of your result object details
- cluster size?
- cluster assignment/membership?
- cluster center?
Plot x colored by the kmeans cluster assignment and
add cluster centers as blue points
```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```


```{r}
km <- kmeans(x, centers=2, nstart=20)
plot(x, col = km$cluster)
points(km$centers, col = "blue", cex = 2, pch = 15)
```


##Hierarchical Clustering
```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc
```

```{r}
# function. Lets make sure we understand it first
dist_matrix <- dist(x)
dim(dist_matrix)
View( as.matrix(dist_matrix) )
dim(x)
dim( as.matrix(dist_matrix) )
```

```{r}
# Create hierarchical cluster model: hc
hc <- hclust(dist(x))
# We can plot the results as a dendrogram
plot(hc)

abline(h=10, col="red")
# Cut by height h
cutree(hc, h=10) 
```

```{r}
# Cut by 2 groups
cutree(hc, k=2) 
```


plotting with differebt linkage
```{r}
# Using different hierarchical clustering methods
d <- dist_matrix

hc.complete <- hclust(d, method="complete")

hc.average <- hclust(d, method="average")

hc.single <- hclust(d, method="single")
```

```{r}
plot(hc.single)
plot(hc.average)
plot(hc.complete)
```




```{r}
# Step 1. Generate some example data for clustering
x <- rbind(matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
  rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


```{r}
hc <- hclust(dist(x))
plot(hc)
cutree(hc, k=2)
abline(h=2, col="red")
abline(h=2.5, col="blue")

```

```{r}
plot(x, col = cutree(hc, ))
```

```{r}
plot(x, col=cutree(hc, k=3))
```




#Practice with PCA
```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",row.names=1)
head(mydata)

pca <- prcomp(t(mydata), scale=TRUE)
pca
```

```{r}
#plot pca 
plot(pca$x[,1], pca$x[,2])
```

```{r}
## Variance captured per PC
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot",
xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
xlab=paste0("PC1 (", pca.var.per[1], "%)"),
ylab=paste0("PC2 (", pca.var.per[2], "%)"))
## Click to identify which sample is which
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
```


